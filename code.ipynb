{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hjc3iIihKLn-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "from dgl.data import DGLDataset\n",
        "import dgl\n",
        "import time\n",
        "import networkx as nx\n",
        "import category_encoders as ce\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl.function as fn\n",
        "import torch\n",
        "import tqdm\n",
        "import math\n",
        "\n",
        "from typing import *\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "import socket\n",
        "import struct\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SvWHb_BpKsLq"
      },
      "outputs": [],
      "source": [
        "file_name = \"NF-CSE-CIC-IDS2018-v2.csv\"\n",
        "data = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "fqly1y-LMwYS",
        "outputId": "18cca6c8-8a93-46c4-ffa1-9d21ebe843b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Label\n",
              "0    16635567\n",
              "1     2258141\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.Label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3t4OREvSM33h"
      },
      "outputs": [],
      "source": [
        "data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "data['IPV4_SRC_ADDR'] = data[\"IPV4_SRC_ADDR\"].apply(str)\n",
        "data['L4_SRC_PORT'] = data[\"L4_SRC_PORT\"].apply(str)\n",
        "data['IPV4_DST_ADDR'] = data[\"IPV4_DST_ADDR\"].apply(str)\n",
        "data['L4_DST_PORT'] = data[\"L4_DST_PORT\"].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bTtHq0XqNXxI"
      },
      "outputs": [],
      "source": [
        "data.drop(columns=[\"L4_SRC_PORT\", \"L4_DST_PORT\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUNIP-8zNkn9",
        "outputId": "34de637f-b644-4249-c3fd-cd19bb3bbde2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['SSH-Bruteforce', 'Benign', 'DDoS attacks-LOIC-HTTP',\n",
              "       'DDOS attack-HOIC', 'DoS attacks-Slowloris', 'DoS attacks-Hulk',\n",
              "       'FTP-BruteForce', 'Infilteration', 'Bot', 'DoS attacks-GoldenEye',\n",
              "       'Brute Force -Web', 'DoS attacks-SlowHTTPTest', 'SQL Injection',\n",
              "       'DDOS attack-LOIC-UDP', 'Brute Force -XSS'], dtype=object)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.Attack.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AlPa58fVN7gB"
      },
      "outputs": [],
      "source": [
        "data = data.groupby(by='Attack').sample(frac=0.1, random_state=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "lcfAP6ViOp-J",
        "outputId": "3641324e-a78b-46bb-c3a4-8af04a7f9449"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IPV4_SRC_ADDR</th>\n",
              "      <th>IPV4_DST_ADDR</th>\n",
              "      <th>PROTOCOL</th>\n",
              "      <th>L7_PROTO</th>\n",
              "      <th>IN_BYTES</th>\n",
              "      <th>IN_PKTS</th>\n",
              "      <th>OUT_BYTES</th>\n",
              "      <th>OUT_PKTS</th>\n",
              "      <th>TCP_FLAGS</th>\n",
              "      <th>CLIENT_TCP_FLAGS</th>\n",
              "      <th>...</th>\n",
              "      <th>NUM_PKTS_1024_TO_1514_BYTES</th>\n",
              "      <th>TCP_WIN_MAX_IN</th>\n",
              "      <th>TCP_WIN_MAX_OUT</th>\n",
              "      <th>ICMP_TYPE</th>\n",
              "      <th>ICMP_IPV4_TYPE</th>\n",
              "      <th>DNS_QUERY_ID</th>\n",
              "      <th>DNS_QUERY_TYPE</th>\n",
              "      <th>DNS_TTL_ANSWER</th>\n",
              "      <th>FTP_COMMAND_RET_CODE</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Attack</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Benign</th>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>...</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "      <td>1663557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bot</th>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>...</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "      <td>14310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brute Force -Web</th>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>...</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brute Force -XSS</th>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>...</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DDOS attack-HOIC</th>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>...</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "      <td>108086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DDOS attack-LOIC-UDP</th>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>...</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DDoS attacks-LOIC-HTTP</th>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>...</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS attacks-GoldenEye</th>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>...</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "      <td>2772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS attacks-Hulk</th>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>...</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "      <td>43265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS attacks-SlowHTTPTest</th>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>...</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "      <td>1412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DoS attacks-Slowloris</th>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>...</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "      <td>951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTP-BruteForce</th>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>...</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "      <td>2593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Infilteration</th>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>...</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "      <td>11636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SQL Injection</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SSH-Bruteforce</th>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>...</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "      <td>9498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          IPV4_SRC_ADDR  IPV4_DST_ADDR  PROTOCOL  L7_PROTO  \\\n",
              "Attack                                                                       \n",
              "Benign                          1663557        1663557   1663557   1663557   \n",
              "Bot                               14310          14310     14310     14310   \n",
              "Brute Force -Web                    214            214       214       214   \n",
              "Brute Force -XSS                     93             93        93        93   \n",
              "DDOS attack-HOIC                 108086         108086    108086    108086   \n",
              "DDOS attack-LOIC-UDP                211            211       211       211   \n",
              "DDoS attacks-LOIC-HTTP            30730          30730     30730     30730   \n",
              "DoS attacks-GoldenEye              2772           2772      2772      2772   \n",
              "DoS attacks-Hulk                  43265          43265     43265     43265   \n",
              "DoS attacks-SlowHTTPTest           1412           1412      1412      1412   \n",
              "DoS attacks-Slowloris               951            951       951       951   \n",
              "FTP-BruteForce                     2593           2593      2593      2593   \n",
              "Infilteration                     11636          11636     11636     11636   \n",
              "SQL Injection                        43             43        43        43   \n",
              "SSH-Bruteforce                     9498           9498      9498      9498   \n",
              "\n",
              "                          IN_BYTES  IN_PKTS  OUT_BYTES  OUT_PKTS  TCP_FLAGS  \\\n",
              "Attack                                                                        \n",
              "Benign                     1663557  1663557    1663557   1663557    1663557   \n",
              "Bot                          14310    14310      14310     14310      14310   \n",
              "Brute Force -Web               214      214        214       214        214   \n",
              "Brute Force -XSS                93       93         93        93         93   \n",
              "DDOS attack-HOIC            108086   108086     108086    108086     108086   \n",
              "DDOS attack-LOIC-UDP           211      211        211       211        211   \n",
              "DDoS attacks-LOIC-HTTP       30730    30730      30730     30730      30730   \n",
              "DoS attacks-GoldenEye         2772     2772       2772      2772       2772   \n",
              "DoS attacks-Hulk             43265    43265      43265     43265      43265   \n",
              "DoS attacks-SlowHTTPTest      1412     1412       1412      1412       1412   \n",
              "DoS attacks-Slowloris          951      951        951       951        951   \n",
              "FTP-BruteForce                2593     2593       2593      2593       2593   \n",
              "Infilteration                11636    11636      11636     11636      11636   \n",
              "SQL Injection                   43       43         43        43         43   \n",
              "SSH-Bruteforce                9498     9498       9498      9498       9498   \n",
              "\n",
              "                          CLIENT_TCP_FLAGS  ...  NUM_PKTS_1024_TO_1514_BYTES  \\\n",
              "Attack                                      ...                                \n",
              "Benign                             1663557  ...                      1663557   \n",
              "Bot                                  14310  ...                        14310   \n",
              "Brute Force -Web                       214  ...                          214   \n",
              "Brute Force -XSS                        93  ...                           93   \n",
              "DDOS attack-HOIC                    108086  ...                       108086   \n",
              "DDOS attack-LOIC-UDP                   211  ...                          211   \n",
              "DDoS attacks-LOIC-HTTP               30730  ...                        30730   \n",
              "DoS attacks-GoldenEye                 2772  ...                         2772   \n",
              "DoS attacks-Hulk                     43265  ...                        43265   \n",
              "DoS attacks-SlowHTTPTest              1412  ...                         1412   \n",
              "DoS attacks-Slowloris                  951  ...                          951   \n",
              "FTP-BruteForce                        2593  ...                         2593   \n",
              "Infilteration                        11636  ...                        11636   \n",
              "SQL Injection                           43  ...                           43   \n",
              "SSH-Bruteforce                        9498  ...                         9498   \n",
              "\n",
              "                          TCP_WIN_MAX_IN  TCP_WIN_MAX_OUT  ICMP_TYPE  \\\n",
              "Attack                                                                 \n",
              "Benign                           1663557          1663557    1663557   \n",
              "Bot                                14310            14310      14310   \n",
              "Brute Force -Web                     214              214        214   \n",
              "Brute Force -XSS                      93               93         93   \n",
              "DDOS attack-HOIC                  108086           108086     108086   \n",
              "DDOS attack-LOIC-UDP                 211              211        211   \n",
              "DDoS attacks-LOIC-HTTP             30730            30730      30730   \n",
              "DoS attacks-GoldenEye               2772             2772       2772   \n",
              "DoS attacks-Hulk                   43265            43265      43265   \n",
              "DoS attacks-SlowHTTPTest            1412             1412       1412   \n",
              "DoS attacks-Slowloris                951              951        951   \n",
              "FTP-BruteForce                      2593             2593       2593   \n",
              "Infilteration                      11636            11636      11636   \n",
              "SQL Injection                         43               43         43   \n",
              "SSH-Bruteforce                      9498             9498       9498   \n",
              "\n",
              "                          ICMP_IPV4_TYPE  DNS_QUERY_ID  DNS_QUERY_TYPE  \\\n",
              "Attack                                                                   \n",
              "Benign                           1663557       1663557         1663557   \n",
              "Bot                                14310         14310           14310   \n",
              "Brute Force -Web                     214           214             214   \n",
              "Brute Force -XSS                      93            93              93   \n",
              "DDOS attack-HOIC                  108086        108086          108086   \n",
              "DDOS attack-LOIC-UDP                 211           211             211   \n",
              "DDoS attacks-LOIC-HTTP             30730         30730           30730   \n",
              "DoS attacks-GoldenEye               2772          2772            2772   \n",
              "DoS attacks-Hulk                   43265         43265           43265   \n",
              "DoS attacks-SlowHTTPTest            1412          1412            1412   \n",
              "DoS attacks-Slowloris                951           951             951   \n",
              "FTP-BruteForce                      2593          2593            2593   \n",
              "Infilteration                      11636         11636           11636   \n",
              "SQL Injection                         43            43              43   \n",
              "SSH-Bruteforce                      9498          9498            9498   \n",
              "\n",
              "                          DNS_TTL_ANSWER  FTP_COMMAND_RET_CODE    Label  \n",
              "Attack                                                                   \n",
              "Benign                           1663557               1663557  1663557  \n",
              "Bot                                14310                 14310    14310  \n",
              "Brute Force -Web                     214                   214      214  \n",
              "Brute Force -XSS                      93                    93       93  \n",
              "DDOS attack-HOIC                  108086                108086   108086  \n",
              "DDOS attack-LOIC-UDP                 211                   211      211  \n",
              "DDoS attacks-LOIC-HTTP             30730                 30730    30730  \n",
              "DoS attacks-GoldenEye               2772                  2772     2772  \n",
              "DoS attacks-Hulk                   43265                 43265    43265  \n",
              "DoS attacks-SlowHTTPTest            1412                  1412     1412  \n",
              "DoS attacks-Slowloris                951                   951      951  \n",
              "FTP-BruteForce                      2593                  2593     2593  \n",
              "Infilteration                      11636                 11636    11636  \n",
              "SQL Injection                         43                    43       43  \n",
              "SSH-Bruteforce                      9498                  9498     9498  \n",
              "\n",
              "[15 rows x 42 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.groupby(by=\"Attack\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FqRx5xCPOuv8"
      },
      "outputs": [],
      "source": [
        "X = data.drop(columns=[\"Attack\", \"Label\"])\n",
        "y = data[[\"Attack\", \"Label\"]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=13, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bPfakXplPGGx"
      },
      "outputs": [],
      "source": [
        "encoder = ce.TargetEncoder(cols=['TCP_FLAGS','L7_PROTO','PROTOCOL',\n",
        "                                  'CLIENT_TCP_FLAGS','SERVER_TCP_FLAGS','ICMP_TYPE',\n",
        "                                  'ICMP_IPV4_TYPE','DNS_QUERY_ID','DNS_QUERY_TYPE',\n",
        "                                  'FTP_COMMAND_RET_CODE'])\n",
        "encoder.fit(X_train, y_train.Label)\n",
        "\n",
        "# Transform on training set\n",
        "X_train = encoder.transform(X_train)\n",
        "\n",
        "# Transform on testing set\n",
        "X_test = encoder.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ibyOfV-8PouK"
      },
      "outputs": [],
      "source": [
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_train.fillna(0, inplace=True)\n",
        "X_test.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "asDnsSIWPee0"
      },
      "outputs": [],
      "source": [
        "scaler = Normalizer()\n",
        "cols_to_norm = list(set(list(X_train.iloc[:, 2:].columns))) # Ignore first two as the represents IP addresses\n",
        "scaler.fit(X_train[cols_to_norm])\n",
        "\n",
        "# Transform on training set\n",
        "X_train[cols_to_norm] = scaler.transform(X_train[cols_to_norm])\n",
        "X_train['h'] = X_train.iloc[:, 2:].values.tolist()\n",
        "\n",
        "# Transform on testing set\n",
        "X_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\n",
        "X_test['h'] = X_test.iloc[:, 2:].values.tolist()\n",
        "\n",
        "train = pd.concat([X_train, y_train], axis=1)\n",
        "test = pd.concat([X_test, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "hErQbsnrPluV",
        "outputId": "c4dbe223-89d5-48f5-800d-16512a77e66c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IPV4_SRC_ADDR</th>\n",
              "      <th>IPV4_DST_ADDR</th>\n",
              "      <th>PROTOCOL</th>\n",
              "      <th>L7_PROTO</th>\n",
              "      <th>IN_BYTES</th>\n",
              "      <th>IN_PKTS</th>\n",
              "      <th>OUT_BYTES</th>\n",
              "      <th>OUT_PKTS</th>\n",
              "      <th>TCP_FLAGS</th>\n",
              "      <th>CLIENT_TCP_FLAGS</th>\n",
              "      <th>...</th>\n",
              "      <th>NUM_PKTS_1024_TO_1514_BYTES</th>\n",
              "      <th>TCP_WIN_MAX_IN</th>\n",
              "      <th>TCP_WIN_MAX_OUT</th>\n",
              "      <th>ICMP_TYPE</th>\n",
              "      <th>ICMP_IPV4_TYPE</th>\n",
              "      <th>DNS_QUERY_ID</th>\n",
              "      <th>DNS_QUERY_TYPE</th>\n",
              "      <th>DNS_TTL_ANSWER</th>\n",
              "      <th>FTP_COMMAND_RET_CODE</th>\n",
              "      <th>h</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10660988</th>\n",
              "      <td>172.31.66.109</td>\n",
              "      <td>172.31.0.2</td>\n",
              "      <td>1.401126e-08</td>\n",
              "      <td>1.108397e-08</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>1.237078e-06</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>1.237078e-06</td>\n",
              "      <td>1.405050e-08</td>\n",
              "      <td>1.405050e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.458031e-07</td>\n",
              "      <td>1.458029e-07</td>\n",
              "      <td>1.136291e-07</td>\n",
              "      <td>1.109630e-08</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>1.478535e-07</td>\n",
              "      <td>[1.4011259652278659e-08, 1.1083974799736558e-0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15147714</th>\n",
              "      <td>172.31.66.68</td>\n",
              "      <td>172.31.0.2</td>\n",
              "      <td>1.245158e-08</td>\n",
              "      <td>1.008341e-08</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>1.099371e-06</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>1.099371e-06</td>\n",
              "      <td>1.248645e-08</td>\n",
              "      <td>1.248645e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.295728e-07</td>\n",
              "      <td>1.295727e-07</td>\n",
              "      <td>9.858040e-08</td>\n",
              "      <td>9.861101e-09</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>1.313950e-07</td>\n",
              "      <td>[1.245157764089544e-08, 1.0083409899455166e-08...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9587672</th>\n",
              "      <td>172.31.64.28</td>\n",
              "      <td>172.31.0.2</td>\n",
              "      <td>1.552131e-08</td>\n",
              "      <td>1.227854e-08</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>1.370403e-06</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>1.370403e-06</td>\n",
              "      <td>1.556478e-08</td>\n",
              "      <td>1.556478e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.615169e-07</td>\n",
              "      <td>1.615167e-07</td>\n",
              "      <td>1.164452e-07</td>\n",
              "      <td>1.229219e-08</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>1.637882e-07</td>\n",
              "      <td>[1.5521310572476723e-08, 1.2278540225056638e-0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7746586</th>\n",
              "      <td>172.31.67.18</td>\n",
              "      <td>172.31.0.2</td>\n",
              "      <td>7.552795e-09</td>\n",
              "      <td>5.974837e-09</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>6.668492e-07</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>6.668492e-07</td>\n",
              "      <td>7.573948e-09</td>\n",
              "      <td>7.573948e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.859541e-08</td>\n",
              "      <td>7.859535e-08</td>\n",
              "      <td>5.499144e-08</td>\n",
              "      <td>5.883835e-09</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>7.970069e-08</td>\n",
              "      <td>[7.55279530717906e-09, 5.974837018934741e-09, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693958</th>\n",
              "      <td>172.31.67.31</td>\n",
              "      <td>172.31.0.2</td>\n",
              "      <td>3.657431e-09</td>\n",
              "      <td>2.893307e-09</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>6.458418e-07</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>6.458418e-07</td>\n",
              "      <td>3.667675e-09</td>\n",
              "      <td>3.667675e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.805972e-08</td>\n",
              "      <td>3.805969e-08</td>\n",
              "      <td>2.821517e-08</td>\n",
              "      <td>2.896524e-09</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>3.859495e-08</td>\n",
              "      <td>[3.6574312874776476e-09, 2.8933070422099717e-0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          IPV4_SRC_ADDR IPV4_DST_ADDR      PROTOCOL      L7_PROTO  IN_BYTES  \\\n",
              "10660988  172.31.66.109    172.31.0.2  1.401126e-08  1.108397e-08  0.000078   \n",
              "15147714   172.31.66.68    172.31.0.2  1.245158e-08  1.008341e-08  0.000079   \n",
              "9587672    172.31.64.28    172.31.0.2  1.552131e-08  1.227854e-08  0.000077   \n",
              "7746586    172.31.67.18    172.31.0.2  7.552795e-09  5.974837e-09  0.000045   \n",
              "693958     172.31.67.31    172.31.0.2  3.657431e-09  2.893307e-09  0.000049   \n",
              "\n",
              "               IN_PKTS  OUT_BYTES      OUT_PKTS     TCP_FLAGS  \\\n",
              "10660988  1.237078e-06   0.000098  1.237078e-06  1.405050e-08   \n",
              "15147714  1.099371e-06   0.000097  1.099371e-06  1.248645e-08   \n",
              "9587672   1.370403e-06   0.000099  1.370403e-06  1.556478e-08   \n",
              "7746586   6.668492e-07   0.000086  6.668492e-07  7.573948e-09   \n",
              "693958    6.458418e-07   0.000115  6.458418e-07  3.667675e-09   \n",
              "\n",
              "          CLIENT_TCP_FLAGS  ...  NUM_PKTS_1024_TO_1514_BYTES  TCP_WIN_MAX_IN  \\\n",
              "10660988      1.405050e-08  ...                          0.0             0.0   \n",
              "15147714      1.248645e-08  ...                          0.0             0.0   \n",
              "9587672       1.556478e-08  ...                          0.0             0.0   \n",
              "7746586       7.573948e-09  ...                          0.0             0.0   \n",
              "693958        3.667675e-09  ...                          0.0             0.0   \n",
              "\n",
              "          TCP_WIN_MAX_OUT     ICMP_TYPE  ICMP_IPV4_TYPE  DNS_QUERY_ID  \\\n",
              "10660988              0.0  1.458031e-07    1.458029e-07  1.136291e-07   \n",
              "15147714              0.0  1.295728e-07    1.295727e-07  9.858040e-08   \n",
              "9587672               0.0  1.615169e-07    1.615167e-07  1.164452e-07   \n",
              "7746586               0.0  7.859541e-08    7.859535e-08  5.499144e-08   \n",
              "693958                0.0  3.805972e-08    3.805969e-08  2.821517e-08   \n",
              "\n",
              "          DNS_QUERY_TYPE  DNS_TTL_ANSWER  FTP_COMMAND_RET_CODE  \\\n",
              "10660988    1.109630e-08        0.000074          1.478535e-07   \n",
              "15147714    9.861101e-09        0.000066          1.313950e-07   \n",
              "9587672     1.229219e-08        0.000082          1.637882e-07   \n",
              "7746586     5.883835e-09        0.000040          7.970069e-08   \n",
              "693958      2.896524e-09        0.000010          3.859495e-08   \n",
              "\n",
              "                                                          h  \n",
              "10660988  [1.4011259652278659e-08, 1.1083974799736558e-0...  \n",
              "15147714  [1.245157764089544e-08, 1.0083409899455166e-08...  \n",
              "9587672   [1.5521310572476723e-08, 1.2278540225056638e-0...  \n",
              "7746586   [7.55279530717906e-09, 5.974837018934741e-09, ...  \n",
              "693958    [3.6574312874776476e-09, 2.8933070422099717e-0...  \n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d_tLtK4WPtrF"
      },
      "outputs": [],
      "source": [
        "lab_enc = preprocessing.LabelEncoder()\n",
        "lab_enc.fit(data[\"Attack\"])\n",
        "\n",
        "# Transform on training set\n",
        "train[\"Attack\"] = lab_enc.transform(train[\"Attack\"])\n",
        "\n",
        "# Transform on testing set\n",
        "test[\"Attack\"] = lab_enc.transform(test[\"Attack\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8yaicjecP1fZ"
      },
      "outputs": [],
      "source": [
        "# Training graph\n",
        "\n",
        "train_g = nx.from_pandas_edgelist(train, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\",\n",
        "            [\"h\", \"Label\", \"Attack\"], create_using=nx.MultiGraph())\n",
        "\n",
        "train_g = train_g.to_directed()\n",
        "train_g = dgl.from_networkx(train_g, edge_attrs=['h', 'Attack', 'Label'])\n",
        "nfeat_weight = torch.ones([train_g.number_of_nodes(),\n",
        "train_g.edata['h'].shape[1]])\n",
        "train_g.ndata['h'] = nfeat_weight\n",
        "\n",
        "# Testing graph\n",
        "test_g = nx.from_pandas_edgelist(test, \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\",\n",
        "            [\"h\", \"Label\", \"Attack\"], create_using=nx.MultiGraph())\n",
        "\n",
        "test_g = test_g.to_directed()\n",
        "test_g = dgl.from_networkx(test_g, edge_attrs=['h', 'Attack', 'Label'])\n",
        "nfeat_weight = torch.ones([test_g.number_of_nodes(),\n",
        "test_g.edata['h'].shape[1]])\n",
        "test_g.ndata['h'] = nfeat_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PUV6DgJ9QRaP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl.function as fn\n",
        "import tqdm\n",
        "import gc\n",
        "\n",
        "class SAGELayer(nn.Module):\n",
        "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
        "      super(SAGELayer, self).__init__()\n",
        "      self.W_apply = nn.Linear(ndim_in + edims , ndim_out)\n",
        "      self.activation = F.relu\n",
        "      self.W_edge = nn.Linear(128 * 2, 256)\n",
        "      self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      gain = nn.init.calculate_gain('relu')\n",
        "      nn.init.xavier_uniform_(self.W_apply.weight, gain=gain)\n",
        "\n",
        "    def message_func(self, edges):\n",
        "      return {'m':  edges.data['h']}\n",
        "\n",
        "    def forward(self, g_dgl, nfeats, efeats):\n",
        "      with g_dgl.local_scope():\n",
        "        g = g_dgl\n",
        "        g.ndata['h'] = nfeats\n",
        "        g.edata['h'] = efeats\n",
        "        g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
        "        g.ndata['h'] = F.relu(self.W_apply(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
        "\n",
        "        # Compute edge embeddings\n",
        "        u, v = g.edges()\n",
        "        edge = self.W_edge(torch.cat((g.srcdata['h'][u], g.dstdata['h'][v]), 2))\n",
        "        return g.ndata['h'], edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_xo-3K4QRGqc"
      },
      "outputs": [],
      "source": [
        "class SAGE(nn.Module):\n",
        "    def __init__(self, ndim_in, ndim_out, edim,  activation):\n",
        "      super(SAGE, self).__init__()\n",
        "      self.layers = nn.ModuleList()\n",
        "      self.layers.append(SAGELayer(ndim_in, edim, 128, F.relu))\n",
        "\n",
        "    def forward(self, g, nfeats, efeats, corrupt=False):\n",
        "      if corrupt:\n",
        "        e_perm = torch.randperm(g.number_of_edges())\n",
        "        #n_perm = torch.randperm(g.number_of_nodes())\n",
        "        efeats = efeats[e_perm]\n",
        "        #nfeats = nfeats[n_perm]\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        #nfeats = layer(g, nfeats, efeats)\n",
        "        nfeats, e_feats = layer(g, nfeats, efeats)\n",
        "      #return nfeats.sum(1)\n",
        "      return nfeats.sum(1), e_feats.sum(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6uuxRtLuRJQL"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_hidden):\n",
        "      super(Discriminator, self).__init__()\n",
        "      self.weight = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "      self.reset_parameters()\n",
        "\n",
        "    def uniform(self, size, tensor):\n",
        "      bound = 1.0 / math.sqrt(size)\n",
        "      if tensor is not None:\n",
        "        tensor.data.uniform_(-bound, bound)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      size = self.weight.size(0)\n",
        "      self.uniform(size, self.weight)\n",
        "\n",
        "    def forward(self, features, summary):\n",
        "      features = torch.matmul(features, torch.matmul(self.weight, summary))\n",
        "      return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZPbVjlCyRUco"
      },
      "outputs": [],
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, ndim_in, ndim_out, edim, activation):\n",
        "      super(DGI, self).__init__()\n",
        "      self.encoder = SAGE(ndim_in, ndim_out, edim,  F.relu)\n",
        "      #self.discriminator = Discriminator(128)\n",
        "      self.discriminator = Discriminator(256)\n",
        "      self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, g, n_features, e_features):\n",
        "      positive = self.encoder(g, n_features, e_features, corrupt=False)\n",
        "      negative = self.encoder(g, n_features, e_features, corrupt=True)\n",
        "      self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, g, n_features, e_features):\n",
        "      positive = self.encoder(g, n_features, e_features, corrupt=False)\n",
        "      negative = self.encoder(g, n_features, e_features, corrupt=True)\n",
        "\n",
        "      positive = positive[1]\n",
        "      negative = negative[1]\n",
        "\n",
        "      summary = torch.sigmoid(positive.mean(dim=0))\n",
        "\n",
        "      positive = self.discriminator(positive, summary)\n",
        "      negative = self.discriminator(negative, summary)\n",
        "\n",
        "      l1 = self.loss(positive, torch.ones_like(positive))\n",
        "      l2 = self.loss(negative, torch.zeros_like(negative))\n",
        "\n",
        "      return l1 + l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sKnfpWFMR19u"
      },
      "outputs": [],
      "source": [
        "ndim_in = train_g.ndata['h'].shape[1]\n",
        "hidden_features = 128\n",
        "ndim_out = 128\n",
        "num_layers = 1\n",
        "edim = train_g.edata['h'].shape[1]\n",
        "learning_rate = 1e-3\n",
        "epochs = 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aSl_9qY8SbA0"
      },
      "outputs": [],
      "source": [
        "dgi = DGI(ndim_in,\n",
        "    ndim_out,\n",
        "    edim,\n",
        "    F.relu)\n",
        "\n",
        "dgi_optimizer = torch.optim.Adam(dgi.parameters(),\n",
        "                lr=1e-3,\n",
        "                weight_decay=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9K6_cOiWSdJA"
      },
      "outputs": [],
      "source": [
        "# Format node and edge features for E-GraphSAGE\n",
        "train_g.ndata['h'] = torch.reshape(train_g.ndata['h'],\n",
        "                                   (train_g.ndata['h'].shape[0], 1,\n",
        "                                    train_g.ndata['h'].shape[1]))\n",
        "\n",
        "train_g.edata['h'] = torch.reshape(train_g.edata['h'],\n",
        "                                   (train_g.edata['h'].shape[0], 1,\n",
        "                                    train_g.edata['h'].shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O44auIyWSexg"
      },
      "outputs": [],
      "source": [
        "# Convert to GPU\n",
        "train_g = train_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.3785, grad_fn=<AddBackward0>)\n",
            "Epoch 00000 | Time(s) nan | Loss 1.3785 | ETputs(KTEPS) nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.3745, grad_fn=<AddBackward0>)\n",
            "Epoch 00001 | Time(s) nan | Loss 1.3745 | ETputs(KTEPS) nan\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00002 | Time(s) nan | Loss 1.3739 | ETputs(KTEPS) nan\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00003 | Time(s) 10.4079 | Loss 1.3774 | ETputs(KTEPS) 253.80\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00004 | Time(s) 10.3726 | Loss 1.3812 | ETputs(KTEPS) 254.67\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00005 | Time(s) 10.4908 | Loss 1.3829 | ETputs(KTEPS) 251.80\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00006 | Time(s) 10.5892 | Loss 1.3817 | ETputs(KTEPS) 249.46\n",
            "tensor(1.3739, grad_fn=<AddBackward0>)\n",
            "Epoch 00007 | Time(s) 10.5528 | Loss 1.3784 | ETputs(KTEPS) 250.32\n",
            "tensor(1.3736, grad_fn=<AddBackward0>)\n",
            "Epoch 00008 | Time(s) 10.4847 | Loss 1.3736 | ETputs(KTEPS) 251.94\n",
            "tensor(1.3686, grad_fn=<AddBackward0>)\n",
            "Epoch 00009 | Time(s) 10.5229 | Loss 1.3686 | ETputs(KTEPS) 251.03\n",
            "tensor(1.3650, grad_fn=<AddBackward0>)\n",
            "Epoch 00010 | Time(s) 10.5260 | Loss 1.3650 | ETputs(KTEPS) 250.95\n",
            "tensor(1.3626, grad_fn=<AddBackward0>)\n",
            "Epoch 00011 | Time(s) 10.6042 | Loss 1.3626 | ETputs(KTEPS) 249.10\n",
            "tensor(1.3614, grad_fn=<AddBackward0>)\n",
            "Epoch 00012 | Time(s) 10.7054 | Loss 1.3614 | ETputs(KTEPS) 246.75\n",
            "tensor(1.3601, grad_fn=<AddBackward0>)\n",
            "Epoch 00013 | Time(s) 10.7512 | Loss 1.3601 | ETputs(KTEPS) 245.70\n",
            "tensor(1.3601, grad_fn=<AddBackward0>)\n",
            "Epoch 00014 | Time(s) 10.7522 | Loss 1.3636 | ETputs(KTEPS) 245.67\n",
            "tensor(1.3601, grad_fn=<AddBackward0>)\n",
            "Epoch 00015 | Time(s) 10.7141 | Loss 1.3601 | ETputs(KTEPS) 246.55\n",
            "tensor(1.3579, grad_fn=<AddBackward0>)\n",
            "Epoch 00016 | Time(s) 10.6979 | Loss 1.3579 | ETputs(KTEPS) 246.92\n",
            "tensor(1.3557, grad_fn=<AddBackward0>)\n",
            "Epoch 00017 | Time(s) 10.6767 | Loss 1.3557 | ETputs(KTEPS) 247.41\n",
            "tensor(1.3530, grad_fn=<AddBackward0>)\n",
            "Epoch 00018 | Time(s) 10.6346 | Loss 1.3530 | ETputs(KTEPS) 248.39\n",
            "tensor(1.3520, grad_fn=<AddBackward0>)\n",
            "Epoch 00019 | Time(s) 10.6124 | Loss 1.3520 | ETputs(KTEPS) 248.91\n",
            "tensor(1.3500, grad_fn=<AddBackward0>)\n",
            "Epoch 00020 | Time(s) 10.6233 | Loss 1.3500 | ETputs(KTEPS) 248.66\n",
            "tensor(1.3484, grad_fn=<AddBackward0>)\n",
            "Epoch 00021 | Time(s) 10.6117 | Loss 1.3484 | ETputs(KTEPS) 248.93\n",
            "tensor(1.3471, grad_fn=<AddBackward0>)\n",
            "Epoch 00022 | Time(s) 10.6164 | Loss 1.3471 | ETputs(KTEPS) 248.82\n",
            "tensor(1.3451, grad_fn=<AddBackward0>)\n",
            "Epoch 00023 | Time(s) 10.6178 | Loss 1.3451 | ETputs(KTEPS) 248.78\n",
            "tensor(1.3414, grad_fn=<AddBackward0>)\n",
            "Epoch 00024 | Time(s) 10.6194 | Loss 1.3414 | ETputs(KTEPS) 248.75\n",
            "tensor(1.3378, grad_fn=<AddBackward0>)\n",
            "Epoch 00025 | Time(s) 10.6169 | Loss 1.3378 | ETputs(KTEPS) 248.81\n",
            "tensor(1.3339, grad_fn=<AddBackward0>)\n",
            "Epoch 00026 | Time(s) 10.6437 | Loss 1.3339 | ETputs(KTEPS) 248.18\n",
            "tensor(1.3285, grad_fn=<AddBackward0>)\n",
            "Epoch 00027 | Time(s) 10.6461 | Loss 1.3285 | ETputs(KTEPS) 248.12\n",
            "tensor(1.3249, grad_fn=<AddBackward0>)\n",
            "Epoch 00028 | Time(s) 10.6423 | Loss 1.3249 | ETputs(KTEPS) 248.21\n",
            "tensor(1.3211, grad_fn=<AddBackward0>)\n",
            "Epoch 00029 | Time(s) 10.6636 | Loss 1.3211 | ETputs(KTEPS) 247.72\n",
            "tensor(1.3193, grad_fn=<AddBackward0>)\n",
            "Epoch 00030 | Time(s) 10.6905 | Loss 1.3193 | ETputs(KTEPS) 247.09\n",
            "tensor(1.3150, grad_fn=<AddBackward0>)\n",
            "Epoch 00031 | Time(s) 10.7262 | Loss 1.3150 | ETputs(KTEPS) 246.27\n",
            "tensor(1.3109, grad_fn=<AddBackward0>)\n",
            "Epoch 00032 | Time(s) 10.7799 | Loss 1.3109 | ETputs(KTEPS) 245.05\n",
            "tensor(1.3060, grad_fn=<AddBackward0>)\n",
            "Epoch 00033 | Time(s) 10.8403 | Loss 1.3060 | ETputs(KTEPS) 243.68\n",
            "tensor(1.3060, grad_fn=<AddBackward0>)\n",
            "Epoch 00034 | Time(s) 10.8834 | Loss 1.3070 | ETputs(KTEPS) 242.71\n",
            "tensor(1.2983, grad_fn=<AddBackward0>)\n",
            "Epoch 00035 | Time(s) 10.9175 | Loss 1.2983 | ETputs(KTEPS) 241.96\n",
            "tensor(1.2962, grad_fn=<AddBackward0>)\n",
            "Epoch 00036 | Time(s) 10.9496 | Loss 1.2962 | ETputs(KTEPS) 241.25\n",
            "tensor(1.2920, grad_fn=<AddBackward0>)\n",
            "Epoch 00037 | Time(s) 10.9728 | Loss 1.2920 | ETputs(KTEPS) 240.74\n",
            "tensor(1.2860, grad_fn=<AddBackward0>)\n",
            "Epoch 00038 | Time(s) 11.0020 | Loss 1.2860 | ETputs(KTEPS) 240.10\n",
            "tensor(1.2818, grad_fn=<AddBackward0>)\n",
            "Epoch 00039 | Time(s) 11.0314 | Loss 1.2818 | ETputs(KTEPS) 239.46\n",
            "tensor(1.2759, grad_fn=<AddBackward0>)\n",
            "Epoch 00040 | Time(s) 11.0502 | Loss 1.2759 | ETputs(KTEPS) 239.05\n",
            "tensor(1.2676, grad_fn=<AddBackward0>)\n",
            "Epoch 00041 | Time(s) 11.0735 | Loss 1.2676 | ETputs(KTEPS) 238.55\n",
            "tensor(1.2641, grad_fn=<AddBackward0>)\n",
            "Epoch 00042 | Time(s) 11.0988 | Loss 1.2641 | ETputs(KTEPS) 238.00\n",
            "tensor(1.2576, grad_fn=<AddBackward0>)\n",
            "Epoch 00043 | Time(s) 11.1113 | Loss 1.2576 | ETputs(KTEPS) 237.74\n",
            "tensor(1.2490, grad_fn=<AddBackward0>)\n",
            "Epoch 00044 | Time(s) 11.1343 | Loss 1.2490 | ETputs(KTEPS) 237.25\n",
            "tensor(1.2490, grad_fn=<AddBackward0>)\n",
            "Epoch 00045 | Time(s) 11.1510 | Loss 1.2595 | ETputs(KTEPS) 236.89\n",
            "tensor(1.2381, grad_fn=<AddBackward0>)\n",
            "Epoch 00046 | Time(s) 11.1702 | Loss 1.2381 | ETputs(KTEPS) 236.48\n",
            "tensor(1.2303, grad_fn=<AddBackward0>)\n",
            "Epoch 00047 | Time(s) 11.1836 | Loss 1.2303 | ETputs(KTEPS) 236.20\n",
            "tensor(1.2264, grad_fn=<AddBackward0>)\n",
            "Epoch 00048 | Time(s) 11.1999 | Loss 1.2264 | ETputs(KTEPS) 235.86\n",
            "tensor(1.2185, grad_fn=<AddBackward0>)\n",
            "Epoch 00049 | Time(s) 11.2135 | Loss 1.2185 | ETputs(KTEPS) 235.57\n",
            "tensor(1.2081, grad_fn=<AddBackward0>)\n",
            "Epoch 00050 | Time(s) 11.2286 | Loss 1.2081 | ETputs(KTEPS) 235.25\n",
            "tensor(1.2008, grad_fn=<AddBackward0>)\n",
            "Epoch 00051 | Time(s) 11.2421 | Loss 1.2008 | ETputs(KTEPS) 234.97\n",
            "tensor(1.1958, grad_fn=<AddBackward0>)\n",
            "Epoch 00052 | Time(s) 11.2622 | Loss 1.1958 | ETputs(KTEPS) 234.55\n",
            "tensor(1.1842, grad_fn=<AddBackward0>)\n",
            "Epoch 00053 | Time(s) 11.2723 | Loss 1.1842 | ETputs(KTEPS) 234.34\n",
            "tensor(1.1738, grad_fn=<AddBackward0>)\n",
            "Epoch 00054 | Time(s) 11.2904 | Loss 1.1738 | ETputs(KTEPS) 233.96\n",
            "tensor(1.1662, grad_fn=<AddBackward0>)\n",
            "Epoch 00055 | Time(s) 11.3117 | Loss 1.1662 | ETputs(KTEPS) 233.52\n",
            "tensor(1.1524, grad_fn=<AddBackward0>)\n",
            "Epoch 00056 | Time(s) 11.3290 | Loss 1.1524 | ETputs(KTEPS) 233.17\n",
            "tensor(1.1400, grad_fn=<AddBackward0>)\n",
            "Epoch 00057 | Time(s) 11.3402 | Loss 1.1400 | ETputs(KTEPS) 232.94\n",
            "tensor(1.1289, grad_fn=<AddBackward0>)\n",
            "Epoch 00058 | Time(s) 11.3673 | Loss 1.1289 | ETputs(KTEPS) 232.38\n",
            "tensor(1.1227, grad_fn=<AddBackward0>)\n",
            "Epoch 00059 | Time(s) 11.3945 | Loss 1.1227 | ETputs(KTEPS) 231.83\n",
            "tensor(1.1094, grad_fn=<AddBackward0>)\n",
            "Epoch 00060 | Time(s) 11.4017 | Loss 1.1094 | ETputs(KTEPS) 231.68\n",
            "tensor(1.1001, grad_fn=<AddBackward0>)\n",
            "Epoch 00061 | Time(s) 11.4161 | Loss 1.1001 | ETputs(KTEPS) 231.39\n",
            "tensor(1.0832, grad_fn=<AddBackward0>)\n",
            "Epoch 00062 | Time(s) 11.4260 | Loss 1.0832 | ETputs(KTEPS) 231.19\n",
            "tensor(1.0750, grad_fn=<AddBackward0>)\n",
            "Epoch 00063 | Time(s) 11.4372 | Loss 1.0750 | ETputs(KTEPS) 230.96\n",
            "tensor(1.0547, grad_fn=<AddBackward0>)\n",
            "Epoch 00064 | Time(s) 11.4413 | Loss 1.0547 | ETputs(KTEPS) 230.88\n",
            "tensor(1.0419, grad_fn=<AddBackward0>)\n",
            "Epoch 00065 | Time(s) 11.4562 | Loss 1.0419 | ETputs(KTEPS) 230.58\n",
            "tensor(1.0208, grad_fn=<AddBackward0>)\n",
            "Epoch 00066 | Time(s) 11.4628 | Loss 1.0208 | ETputs(KTEPS) 230.45\n",
            "tensor(1.0182, grad_fn=<AddBackward0>)\n",
            "Epoch 00067 | Time(s) 11.4688 | Loss 1.0182 | ETputs(KTEPS) 230.32\n",
            "tensor(0.9869, grad_fn=<AddBackward0>)\n",
            "Epoch 00068 | Time(s) 11.4812 | Loss 0.9869 | ETputs(KTEPS) 230.08\n",
            "tensor(0.9600, grad_fn=<AddBackward0>)\n",
            "Epoch 00069 | Time(s) 11.4974 | Loss 0.9600 | ETputs(KTEPS) 229.75\n",
            "tensor(0.9600, grad_fn=<AddBackward0>)\n",
            "Epoch 00070 | Time(s) 11.5036 | Loss 0.9778 | ETputs(KTEPS) 229.63\n",
            "tensor(0.9336, grad_fn=<AddBackward0>)\n",
            "Epoch 00071 | Time(s) 11.5117 | Loss 0.9336 | ETputs(KTEPS) 229.47\n",
            "tensor(0.9208, grad_fn=<AddBackward0>)\n",
            "Epoch 00072 | Time(s) 11.5247 | Loss 0.9208 | ETputs(KTEPS) 229.21\n",
            "tensor(0.9030, grad_fn=<AddBackward0>)\n",
            "Epoch 00073 | Time(s) 11.5294 | Loss 0.9030 | ETputs(KTEPS) 229.11\n",
            "tensor(0.9030, grad_fn=<AddBackward0>)\n",
            "Epoch 00074 | Time(s) 11.5388 | Loss 0.9047 | ETputs(KTEPS) 228.93\n",
            "tensor(0.8794, grad_fn=<AddBackward0>)\n",
            "Epoch 00075 | Time(s) 11.5513 | Loss 0.8794 | ETputs(KTEPS) 228.68\n",
            "tensor(0.8497, grad_fn=<AddBackward0>)\n",
            "Epoch 00076 | Time(s) 11.5587 | Loss 0.8497 | ETputs(KTEPS) 228.53\n",
            "tensor(0.8206, grad_fn=<AddBackward0>)\n",
            "Epoch 00077 | Time(s) 11.5692 | Loss 0.8206 | ETputs(KTEPS) 228.33\n",
            "tensor(0.8140, grad_fn=<AddBackward0>)\n",
            "Epoch 00078 | Time(s) 11.5736 | Loss 0.8140 | ETputs(KTEPS) 228.24\n",
            "tensor(0.7800, grad_fn=<AddBackward0>)\n",
            "Epoch 00079 | Time(s) 11.5764 | Loss 0.7800 | ETputs(KTEPS) 228.19\n",
            "tensor(0.7632, grad_fn=<AddBackward0>)\n",
            "Epoch 00080 | Time(s) 11.5816 | Loss 0.7632 | ETputs(KTEPS) 228.08\n",
            "tensor(0.7632, grad_fn=<AddBackward0>)\n",
            "Epoch 00081 | Time(s) 11.5850 | Loss 0.7896 | ETputs(KTEPS) 228.01\n",
            "tensor(0.7632, grad_fn=<AddBackward0>)\n",
            "Epoch 00082 | Time(s) 11.5925 | Loss 0.8380 | ETputs(KTEPS) 227.87\n",
            "tensor(0.7632, grad_fn=<AddBackward0>)\n",
            "Epoch 00083 | Time(s) 11.6049 | Loss 0.8818 | ETputs(KTEPS) 227.62\n",
            "tensor(0.7518, grad_fn=<AddBackward0>)\n",
            "Epoch 00084 | Time(s) 11.6189 | Loss 0.7518 | ETputs(KTEPS) 227.35\n",
            "tensor(0.6432, grad_fn=<AddBackward0>)\n",
            "Epoch 00085 | Time(s) 11.6316 | Loss 0.6432 | ETputs(KTEPS) 227.10\n",
            "tensor(0.6432, grad_fn=<AddBackward0>)\n",
            "Epoch 00086 | Time(s) 11.6350 | Loss 0.7105 | ETputs(KTEPS) 227.03\n",
            "tensor(0.6432, grad_fn=<AddBackward0>)\n",
            "Epoch 00087 | Time(s) 11.6409 | Loss 0.7617 | ETputs(KTEPS) 226.92\n",
            "tensor(0.6432, grad_fn=<AddBackward0>)\n",
            "Epoch 00088 | Time(s) 11.6417 | Loss 0.6781 | ETputs(KTEPS) 226.90\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00089 | Time(s) 11.6452 | Loss 0.5616 | ETputs(KTEPS) 226.84\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00090 | Time(s) 11.6461 | Loss 0.7160 | ETputs(KTEPS) 226.82\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00091 | Time(s) 11.6473 | Loss 0.9385 | ETputs(KTEPS) 226.80\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00092 | Time(s) 11.6515 | Loss 0.6138 | ETputs(KTEPS) 226.71\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00093 | Time(s) 11.6602 | Loss 0.6736 | ETputs(KTEPS) 226.54\n",
            "tensor(0.5616, grad_fn=<AddBackward0>)\n",
            "Epoch 00094 | Time(s) 11.6659 | Loss 0.8228 | ETputs(KTEPS) 226.43\n",
            "tensor(0.5205, grad_fn=<AddBackward0>)\n",
            "Epoch 00095 | Time(s) 11.6718 | Loss 0.5205 | ETputs(KTEPS) 226.32\n",
            "tensor(0.5205, grad_fn=<AddBackward0>)\n",
            "Epoch 00096 | Time(s) 11.6756 | Loss 0.7857 | ETputs(KTEPS) 226.25\n",
            "tensor(0.5205, grad_fn=<AddBackward0>)\n",
            "Epoch 00097 | Time(s) 11.6820 | Loss 0.6242 | ETputs(KTEPS) 226.12\n",
            "tensor(0.5205, grad_fn=<AddBackward0>)\n",
            "Epoch 00098 | Time(s) 11.6856 | Loss 0.5416 | ETputs(KTEPS) 226.05\n",
            "tensor(0.5205, grad_fn=<AddBackward0>)\n",
            "Epoch 00099 | Time(s) 11.6883 | Loss 0.6291 | ETputs(KTEPS) 226.00\n"
          ]
        }
      ],
      "source": [
        "cnt_wait = 0\n",
        "best = 1e9\n",
        "best_t = 0\n",
        "dur = []\n",
        "node_features = train_g.ndata['h']\n",
        "edge_features = train_g.edata['h']\n",
        "\n",
        "for epoch in range(100):\n",
        "    dgi.train()\n",
        "    if epoch >= 3:\n",
        "        t0 = time.time()\n",
        "\n",
        "    dgi_optimizer.zero_grad()\n",
        "    loss = dgi(train_g, node_features, edge_features)\n",
        "    loss.backward()\n",
        "    dgi_optimizer.step()\n",
        "\n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        best_t = epoch\n",
        "        cnt_wait = 0\n",
        "        torch.save(dgi.state_dict(), 'best_dgi.pkl')\n",
        "    else:\n",
        "        cnt_wait += 1\n",
        "\n",
        "  # if cnt_wait == patience:\n",
        "  #     print('Early stopping!')\n",
        "  #     break\n",
        "\n",
        "    if epoch >= 3:\n",
        "        dur.append(time.time() - t0)\n",
        "    print(best)\n",
        "\n",
        "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | \"\n",
        "            \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur),\n",
        "              loss.item(),\n",
        "              train_g.num_edges() / np.mean(dur) / 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('encoder.layers.0.W_apply.weight', tensor([[-0.0355, -0.0041,  0.1557,  ...,  0.1742,  0.1231, -0.0261],\n",
            "        [ 0.1564,  0.1060,  0.2002,  ...,  0.1972, -0.1495, -0.1820],\n",
            "        [ 0.1513,  0.0872,  0.0805,  ...,  0.1926, -0.1676,  0.2177],\n",
            "        ...,\n",
            "        [-0.0523, -0.1853, -0.1190,  ..., -0.1079,  0.1321, -0.0012],\n",
            "        [ 0.2157, -0.1179, -0.0950,  ...,  0.0720,  0.0948,  0.0848],\n",
            "        [ 0.1668, -0.0985,  0.1407,  ..., -0.0064, -0.2001,  0.1592]])), ('encoder.layers.0.W_apply.bias', tensor([ 1.0232e-01, -8.1989e-02, -7.5615e-02,  9.3513e-02,  6.2228e-02,\n",
            "        -3.4318e-02, -1.5065e-02, -9.4124e-02,  8.4912e-02,  5.7707e-02,\n",
            "         8.1954e-02, -4.2385e-02, -9.4747e-02,  5.3199e-02,  4.8826e-02,\n",
            "        -9.0551e-02,  1.4368e-02, -2.2409e-02, -2.4087e-02,  7.5786e-02,\n",
            "        -6.4988e-02, -1.0907e-01,  4.7972e-02,  7.0384e-03, -7.6541e-02,\n",
            "        -1.1493e-01,  2.7699e-02, -1.0571e-01,  1.8931e-02, -1.1075e-01,\n",
            "         4.5187e-02, -9.9092e-02, -6.5918e-02,  8.2108e-02,  1.2336e-02,\n",
            "        -5.1713e-02, -1.0802e-02,  2.5526e-02, -1.0561e-01,  4.6892e-02,\n",
            "        -5.9838e-02,  4.5793e-02, -9.8212e-02, -1.0991e-01, -8.2270e-02,\n",
            "         7.8498e-02, -9.5285e-02,  9.4452e-02,  1.0352e-01, -1.0784e-01,\n",
            "         7.8893e-02, -2.7266e-02,  1.0382e-01,  7.4330e-02,  4.5149e-02,\n",
            "        -1.0003e-01, -1.0406e-01,  8.8359e-02, -1.0086e-01,  8.6973e-02,\n",
            "        -2.6009e-02, -6.7218e-02, -5.1943e-02,  3.0800e-02,  8.7874e-02,\n",
            "         2.5226e-02, -2.0712e-02,  6.3653e-02, -2.0679e-02,  7.3844e-02,\n",
            "         4.5701e-02, -8.3275e-02, -9.4891e-02,  4.7773e-02,  4.8706e-02,\n",
            "        -9.6423e-02,  7.0438e-02, -1.8007e-02, -3.4452e-02, -8.0183e-02,\n",
            "         2.3047e-02,  1.0198e-01,  4.4318e-02,  1.0304e-02,  5.7173e-03,\n",
            "        -1.1605e-01,  7.7811e-02,  9.3356e-02, -6.4358e-02,  8.4785e-02,\n",
            "         7.6499e-02, -1.4253e-02, -4.8000e-02,  2.5665e-02,  6.4564e-02,\n",
            "        -3.4935e-02, -3.6595e-02, -4.1162e-02,  7.1276e-02,  9.7213e-02,\n",
            "         9.2981e-02, -3.4682e-03, -7.3266e-02, -1.0895e-01,  9.8755e-02,\n",
            "         2.0004e-02, -5.5807e-02, -4.0546e-03,  9.8224e-02, -9.1158e-02,\n",
            "        -4.2709e-02,  1.8717e-03, -1.1824e-01, -1.1006e-01,  1.4554e-03,\n",
            "        -1.1558e-01, -4.9962e-02,  7.3624e-02, -5.4077e-02, -7.3908e-02,\n",
            "         8.1163e-02, -4.9514e-02,  1.8368e-02, -1.0044e-01,  5.8184e-02,\n",
            "         5.4248e-05, -7.7742e-02, -2.9748e-02])), ('encoder.layers.0.W_edge.weight', tensor([[-5.9731e-03, -6.2937e-05, -3.0547e-02,  ..., -5.3363e-02,\n",
            "          6.0863e-03,  3.0205e-02],\n",
            "        [ 3.4949e-03, -6.3322e-02, -3.7216e-02,  ..., -2.0093e-02,\n",
            "         -2.1177e-02, -7.7150e-02],\n",
            "        [-1.9035e-02,  2.5966e-02,  4.7614e-02,  ..., -2.9616e-02,\n",
            "         -4.0236e-02,  5.9876e-02],\n",
            "        ...,\n",
            "        [ 6.9207e-02, -3.7919e-02,  4.1697e-02,  ...,  4.3421e-02,\n",
            "         -5.5556e-02, -4.1868e-02],\n",
            "        [ 1.1763e-02,  4.8041e-02,  6.8898e-02,  ...,  5.9471e-02,\n",
            "         -1.1783e-02,  7.5166e-02],\n",
            "        [ 5.4593e-03, -5.5962e-02, -9.2572e-03,  ...,  2.6406e-02,\n",
            "          3.8132e-02, -2.6146e-03]])), ('encoder.layers.0.W_edge.bias', tensor([-5.6971e-02, -2.3013e-02,  3.5837e-02, -3.0555e-02,  1.8147e-03,\n",
            "         4.2827e-02,  2.1944e-02,  5.4743e-02,  3.5738e-02, -4.4293e-02,\n",
            "         6.6364e-03,  1.0725e-02,  2.5306e-02,  4.5801e-02, -5.0736e-02,\n",
            "         4.1687e-02, -6.0567e-02,  3.7646e-02,  1.4207e-02, -3.6530e-02,\n",
            "         2.4458e-02,  6.2529e-03,  1.3530e-02, -4.3250e-02, -1.0267e-02,\n",
            "         5.2589e-02,  2.2588e-02, -1.5138e-03,  5.7195e-02,  9.1011e-03,\n",
            "        -1.2179e-02, -5.6598e-02, -2.1672e-02,  3.2240e-02, -1.9780e-03,\n",
            "         3.7842e-02, -3.2951e-02,  4.6202e-02,  1.7504e-02,  1.3823e-02,\n",
            "        -1.7105e-02,  1.5187e-02,  3.8854e-02,  2.9759e-03, -2.4344e-02,\n",
            "        -5.9365e-02, -3.4975e-02,  5.8877e-02,  6.3459e-02,  2.0166e-02,\n",
            "         2.6192e-02,  6.6414e-02, -1.6890e-02,  5.9391e-02,  4.2493e-02,\n",
            "         3.3347e-02,  4.5115e-02,  5.6823e-02,  4.7309e-02,  4.2353e-02,\n",
            "        -2.8308e-02,  6.3817e-03, -2.2872e-02, -4.6414e-02,  2.5550e-02,\n",
            "        -3.5192e-02, -5.0869e-02, -4.1297e-03, -2.6582e-02,  3.2226e-05,\n",
            "         5.2374e-02, -2.3711e-02,  2.9167e-03, -5.7313e-02,  2.5239e-02,\n",
            "         5.0907e-02, -8.8040e-03, -1.4593e-02, -6.3287e-03, -2.8586e-02,\n",
            "         5.9599e-02,  9.9323e-04,  2.7952e-02, -1.4612e-02, -5.8084e-02,\n",
            "        -7.3137e-03,  5.6072e-02,  5.1581e-03,  3.8653e-02,  5.0735e-02,\n",
            "        -3.6920e-02, -4.1232e-02,  6.0081e-03,  5.6098e-02,  2.0832e-02,\n",
            "         6.5850e-02,  4.9141e-02,  2.0497e-02, -3.0799e-02,  6.2918e-03,\n",
            "        -1.2253e-02, -5.0552e-02,  3.1111e-02,  1.8112e-02,  2.5242e-02,\n",
            "         3.0310e-02, -1.3324e-02,  2.7292e-02, -2.1948e-02, -5.6800e-02,\n",
            "        -1.9365e-02, -6.1781e-02,  4.4808e-02, -4.5423e-02, -2.0719e-02,\n",
            "        -2.0089e-02, -3.5895e-02, -5.3533e-02, -5.1197e-02,  4.3721e-02,\n",
            "        -3.3835e-02,  4.5916e-02,  2.6292e-02, -3.0383e-03,  2.0155e-02,\n",
            "         5.7851e-02, -4.0035e-02,  2.4667e-02,  4.7760e-03, -1.6373e-02,\n",
            "         2.3801e-02, -5.2944e-02,  4.1348e-02,  2.5554e-02, -5.8808e-02,\n",
            "         3.6137e-02, -3.6594e-02,  6.4765e-02,  1.8118e-02, -2.2481e-02,\n",
            "        -2.2738e-02,  4.8684e-02, -5.0675e-02,  4.1250e-02, -2.4378e-02,\n",
            "        -4.2000e-02,  1.7023e-02,  5.8113e-02, -2.3881e-02, -4.9819e-02,\n",
            "         5.9248e-03,  4.3754e-02, -2.4753e-02,  3.1028e-02,  5.9831e-03,\n",
            "        -1.7734e-02,  2.5549e-02, -9.2563e-03, -5.5659e-02,  3.6883e-02,\n",
            "        -6.9236e-03, -6.1571e-03, -1.5481e-02, -3.6918e-02, -2.2359e-02,\n",
            "        -1.7850e-02, -1.6750e-02,  4.8008e-02,  1.0582e-02,  1.4050e-03,\n",
            "        -4.5150e-02,  1.0589e-02,  5.4786e-02, -2.5899e-02, -1.5177e-02,\n",
            "        -5.4209e-04, -4.6489e-02, -5.4641e-02,  2.4865e-02,  4.5052e-02,\n",
            "        -2.3654e-03, -4.6592e-02,  3.1606e-02, -1.7755e-02, -5.1625e-02,\n",
            "         4.4327e-02,  6.0624e-02, -3.0154e-02, -4.6341e-02,  1.6566e-02,\n",
            "        -4.5444e-02, -4.9577e-02,  1.7414e-02, -3.2549e-02, -4.8543e-02,\n",
            "         2.2668e-02,  7.3255e-03,  4.0306e-02,  6.6509e-03, -5.6235e-02,\n",
            "         3.5356e-03, -3.0089e-02, -1.4655e-02,  5.3221e-02,  5.4841e-02,\n",
            "         3.2198e-02, -4.9204e-02,  5.2404e-02,  2.2249e-02, -5.1607e-02,\n",
            "        -4.9079e-04,  3.4700e-02,  4.3089e-02, -6.3940e-02, -1.7542e-02,\n",
            "        -3.6771e-02, -3.4167e-02, -2.2372e-02, -4.5777e-02, -2.8353e-02,\n",
            "        -8.0003e-03,  1.5155e-02,  3.4764e-02,  4.7979e-02, -7.4831e-03,\n",
            "        -6.1523e-02,  2.3623e-02,  2.4078e-02, -4.4526e-02,  8.9730e-03,\n",
            "        -3.4747e-02,  3.0945e-02,  5.2234e-03, -3.0877e-02, -5.3778e-02,\n",
            "        -3.4387e-02, -3.9932e-02,  4.7366e-02, -2.8357e-02, -1.4438e-02,\n",
            "         3.3568e-02,  4.5176e-02,  2.5153e-03, -3.4837e-02,  4.7826e-02,\n",
            "        -2.9340e-02,  4.2343e-02, -3.1399e-02, -2.9761e-02,  5.3485e-02,\n",
            "         2.7782e-02, -5.6044e-02, -3.6652e-02,  2.4984e-04, -4.8592e-02,\n",
            "        -5.0621e-02])), ('discriminator.weight', tensor([[ 0.0499,  0.0281,  0.0369,  ..., -0.0483,  0.0615, -0.0547],\n",
            "        [-0.0591,  0.0372, -0.0316,  ..., -0.0677, -0.0550, -0.0278],\n",
            "        [-0.0160,  0.0252, -0.0109,  ..., -0.0413,  0.0395, -0.0349],\n",
            "        ...,\n",
            "        [-0.0197,  0.0368,  0.0009,  ...,  0.0427, -0.0199,  0.0423],\n",
            "        [ 0.0652,  0.0615,  0.0056,  ..., -0.0377, -0.0489, -0.0003],\n",
            "        [-0.0479, -0.0041,  0.0028,  ...,  0.0050,  0.0183,  0.0288]]))])\n"
          ]
        }
      ],
      "source": [
        "print(dgi.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6Ek16GkRStKP"
      },
      "outputs": [],
      "source": [
        "training_emb = dgi.encoder(train_g, train_g.ndata['h'], train_g.edata['h'])[1]\n",
        "training_emb = training_emb.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-FwaBlOdS4ep"
      },
      "outputs": [],
      "source": [
        "test_g.ndata['h'] = torch.reshape(test_g.ndata['h'],\n",
        "                                   (test_g.ndata['h'].shape[0], 1,\n",
        "                                    test_g.ndata['h'].shape[1]))\n",
        "\n",
        "\n",
        "\n",
        "test_g.edata['h'] = torch.reshape(test_g.edata['h'],\n",
        "                                   (test_g.edata['h'].shape[0], 1,\n",
        "                                    test_g.edata['h'].shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SBa-rdivS6cQ"
      },
      "outputs": [],
      "source": [
        "# Convert to GPU\n",
        "test_g = test_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "W12WLjslS-kx"
      },
      "outputs": [],
      "source": [
        "testing_emb = dgi.encoder(test_g, test_g.ndata['h'], test_g.edata['h'])[1]\n",
        "testing_emb = testing_emb.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ERsOAMjeS_D8"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(training_emb, )\n",
        "df_train[\"Attack\"] = lab_enc.inverse_transform(\n",
        "        train_g.edata['Attack'].detach().cpu().numpy())\n",
        "df_train[\"Label\"] = train_g.edata['Label'].detach().cpu().numpy()\n",
        "\n",
        "df_test = pd.DataFrame(testing_emb, )\n",
        "df_test[\"Attack\"] = lab_enc.inverse_transform(\n",
        "        test_g.edata['Attack'].detach().cpu().numpy())\n",
        "df_test[\"Label\"] = test_g.edata['Label'].detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "B8p79H9dat5T",
        "outputId": "0d6e82d8-5d02-49eb-a16f-d44e52ea3dff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>Attack</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.089536</td>\n",
              "      <td>0.234762</td>\n",
              "      <td>0.499413</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>-0.108356</td>\n",
              "      <td>-0.442557</td>\n",
              "      <td>-0.095262</td>\n",
              "      <td>0.068685</td>\n",
              "      <td>0.096509</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100570</td>\n",
              "      <td>0.261413</td>\n",
              "      <td>0.077442</td>\n",
              "      <td>-0.006520</td>\n",
              "      <td>0.272711</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>-0.090303</td>\n",
              "      <td>-0.199628</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.089536</td>\n",
              "      <td>0.234762</td>\n",
              "      <td>0.499413</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>-0.108356</td>\n",
              "      <td>-0.442557</td>\n",
              "      <td>-0.095262</td>\n",
              "      <td>0.068685</td>\n",
              "      <td>0.096509</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100570</td>\n",
              "      <td>0.261413</td>\n",
              "      <td>0.077442</td>\n",
              "      <td>-0.006520</td>\n",
              "      <td>0.272711</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>-0.090303</td>\n",
              "      <td>-0.199628</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.089536</td>\n",
              "      <td>0.234762</td>\n",
              "      <td>0.499413</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>-0.108356</td>\n",
              "      <td>-0.442557</td>\n",
              "      <td>-0.095262</td>\n",
              "      <td>0.068685</td>\n",
              "      <td>0.096509</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100570</td>\n",
              "      <td>0.261413</td>\n",
              "      <td>0.077442</td>\n",
              "      <td>-0.006520</td>\n",
              "      <td>0.272711</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>-0.090303</td>\n",
              "      <td>-0.199628</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.089536</td>\n",
              "      <td>0.234762</td>\n",
              "      <td>0.499413</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>-0.108356</td>\n",
              "      <td>-0.442557</td>\n",
              "      <td>-0.095262</td>\n",
              "      <td>0.068685</td>\n",
              "      <td>0.096509</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100570</td>\n",
              "      <td>0.261413</td>\n",
              "      <td>0.077442</td>\n",
              "      <td>-0.006520</td>\n",
              "      <td>0.272711</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>-0.090303</td>\n",
              "      <td>-0.199628</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.089536</td>\n",
              "      <td>0.234762</td>\n",
              "      <td>0.499413</td>\n",
              "      <td>0.610749</td>\n",
              "      <td>-0.108356</td>\n",
              "      <td>-0.442557</td>\n",
              "      <td>-0.095262</td>\n",
              "      <td>0.068685</td>\n",
              "      <td>0.096509</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100570</td>\n",
              "      <td>0.261413</td>\n",
              "      <td>0.077442</td>\n",
              "      <td>-0.006520</td>\n",
              "      <td>0.272711</td>\n",
              "      <td>-0.072337</td>\n",
              "      <td>-0.090303</td>\n",
              "      <td>-0.199628</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2641549</th>\n",
              "      <td>-0.184909</td>\n",
              "      <td>0.099202</td>\n",
              "      <td>0.243210</td>\n",
              "      <td>0.494740</td>\n",
              "      <td>0.596927</td>\n",
              "      <td>-0.109210</td>\n",
              "      <td>-0.437399</td>\n",
              "      <td>-0.103650</td>\n",
              "      <td>0.072978</td>\n",
              "      <td>0.098303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100357</td>\n",
              "      <td>0.261067</td>\n",
              "      <td>0.081300</td>\n",
              "      <td>-0.022048</td>\n",
              "      <td>0.282673</td>\n",
              "      <td>-0.056804</td>\n",
              "      <td>-0.085026</td>\n",
              "      <td>-0.186575</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2641550</th>\n",
              "      <td>-0.193974</td>\n",
              "      <td>0.120719</td>\n",
              "      <td>0.243462</td>\n",
              "      <td>0.516168</td>\n",
              "      <td>0.583101</td>\n",
              "      <td>-0.084616</td>\n",
              "      <td>-0.444571</td>\n",
              "      <td>-0.132814</td>\n",
              "      <td>0.085291</td>\n",
              "      <td>0.093678</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084378</td>\n",
              "      <td>0.268192</td>\n",
              "      <td>0.070342</td>\n",
              "      <td>-0.044484</td>\n",
              "      <td>0.320765</td>\n",
              "      <td>-0.016463</td>\n",
              "      <td>-0.070495</td>\n",
              "      <td>-0.216916</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2641551</th>\n",
              "      <td>-0.191958</td>\n",
              "      <td>0.123398</td>\n",
              "      <td>0.250685</td>\n",
              "      <td>0.501447</td>\n",
              "      <td>0.574978</td>\n",
              "      <td>-0.084962</td>\n",
              "      <td>-0.455707</td>\n",
              "      <td>-0.125891</td>\n",
              "      <td>0.064002</td>\n",
              "      <td>0.079514</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083362</td>\n",
              "      <td>0.268188</td>\n",
              "      <td>0.070823</td>\n",
              "      <td>-0.048042</td>\n",
              "      <td>0.308147</td>\n",
              "      <td>-0.025993</td>\n",
              "      <td>-0.064052</td>\n",
              "      <td>-0.212066</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2641552</th>\n",
              "      <td>-0.193751</td>\n",
              "      <td>0.122689</td>\n",
              "      <td>0.249479</td>\n",
              "      <td>0.508969</td>\n",
              "      <td>0.582178</td>\n",
              "      <td>-0.082512</td>\n",
              "      <td>-0.451045</td>\n",
              "      <td>-0.130019</td>\n",
              "      <td>0.073107</td>\n",
              "      <td>0.084857</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.086211</td>\n",
              "      <td>0.268132</td>\n",
              "      <td>0.068637</td>\n",
              "      <td>-0.048151</td>\n",
              "      <td>0.312106</td>\n",
              "      <td>-0.021339</td>\n",
              "      <td>-0.071330</td>\n",
              "      <td>-0.216649</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2641553</th>\n",
              "      <td>-0.184450</td>\n",
              "      <td>0.097336</td>\n",
              "      <td>0.237362</td>\n",
              "      <td>0.498895</td>\n",
              "      <td>0.594294</td>\n",
              "      <td>-0.112672</td>\n",
              "      <td>-0.431959</td>\n",
              "      <td>-0.105141</td>\n",
              "      <td>0.081464</td>\n",
              "      <td>0.105890</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.097555</td>\n",
              "      <td>0.261004</td>\n",
              "      <td>0.084160</td>\n",
              "      <td>-0.018146</td>\n",
              "      <td>0.289756</td>\n",
              "      <td>-0.053680</td>\n",
              "      <td>-0.080960</td>\n",
              "      <td>-0.184663</td>\n",
              "      <td>Benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2641554 rows × 258 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2         3         4         5         6  \\\n",
              "0       -0.186130  0.089536  0.234762  0.499413  0.610749 -0.108356 -0.442557   \n",
              "1       -0.186130  0.089536  0.234762  0.499413  0.610749 -0.108356 -0.442557   \n",
              "2       -0.186130  0.089536  0.234762  0.499413  0.610749 -0.108356 -0.442557   \n",
              "3       -0.186130  0.089536  0.234762  0.499413  0.610749 -0.108356 -0.442557   \n",
              "4       -0.186130  0.089536  0.234762  0.499413  0.610749 -0.108356 -0.442557   \n",
              "...           ...       ...       ...       ...       ...       ...       ...   \n",
              "2641549 -0.184909  0.099202  0.243210  0.494740  0.596927 -0.109210 -0.437399   \n",
              "2641550 -0.193974  0.120719  0.243462  0.516168  0.583101 -0.084616 -0.444571   \n",
              "2641551 -0.191958  0.123398  0.250685  0.501447  0.574978 -0.084962 -0.455707   \n",
              "2641552 -0.193751  0.122689  0.249479  0.508969  0.582178 -0.082512 -0.451045   \n",
              "2641553 -0.184450  0.097336  0.237362  0.498895  0.594294 -0.112672 -0.431959   \n",
              "\n",
              "                7         8         9  ...       248       249       250  \\\n",
              "0       -0.095262  0.068685  0.096509  ... -0.100570  0.261413  0.077442   \n",
              "1       -0.095262  0.068685  0.096509  ... -0.100570  0.261413  0.077442   \n",
              "2       -0.095262  0.068685  0.096509  ... -0.100570  0.261413  0.077442   \n",
              "3       -0.095262  0.068685  0.096509  ... -0.100570  0.261413  0.077442   \n",
              "4       -0.095262  0.068685  0.096509  ... -0.100570  0.261413  0.077442   \n",
              "...           ...       ...       ...  ...       ...       ...       ...   \n",
              "2641549 -0.103650  0.072978  0.098303  ... -0.100357  0.261067  0.081300   \n",
              "2641550 -0.132814  0.085291  0.093678  ... -0.084378  0.268192  0.070342   \n",
              "2641551 -0.125891  0.064002  0.079514  ... -0.083362  0.268188  0.070823   \n",
              "2641552 -0.130019  0.073107  0.084857  ... -0.086211  0.268132  0.068637   \n",
              "2641553 -0.105141  0.081464  0.105890  ... -0.097555  0.261004  0.084160   \n",
              "\n",
              "              251       252       253       254       255  Attack  Label  \n",
              "0       -0.006520  0.272711 -0.072337 -0.090303 -0.199628  Benign      0  \n",
              "1       -0.006520  0.272711 -0.072337 -0.090303 -0.199628  Benign      0  \n",
              "2       -0.006520  0.272711 -0.072337 -0.090303 -0.199628  Benign      0  \n",
              "3       -0.006520  0.272711 -0.072337 -0.090303 -0.199628  Benign      0  \n",
              "4       -0.006520  0.272711 -0.072337 -0.090303 -0.199628  Benign      0  \n",
              "...           ...       ...       ...       ...       ...     ...    ...  \n",
              "2641549 -0.022048  0.282673 -0.056804 -0.085026 -0.186575  Benign      0  \n",
              "2641550 -0.044484  0.320765 -0.016463 -0.070495 -0.216916  Benign      0  \n",
              "2641551 -0.048042  0.308147 -0.025993 -0.064052 -0.212066  Benign      0  \n",
              "2641552 -0.048151  0.312106 -0.021339 -0.071330 -0.216649  Benign      0  \n",
              "2641553 -0.018146  0.289756 -0.053680 -0.080960 -0.184663  Benign      0  \n",
              "\n",
              "[2641554 rows x 258 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ScEk1y_TzzX"
      },
      "source": [
        "# Embeddings CBLOF  Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZYABKzdrTGas"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import dgl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import gc\n",
        "\n",
        "from tqdm import tqdm\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RkFS_-dcTJeK"
      },
      "outputs": [],
      "source": [
        "benign_train_samples = df_train[df_train.Label == 0].drop(columns=[\"Label\", \"Attack\"])\n",
        "normal_train_samples = df_train.drop(columns=[\"Label\", \"Attack\"])\n",
        "\n",
        "train_labels = df_train[\"Label\"]\n",
        "test_labels = df_test[\"Label\"]\n",
        "\n",
        "test_samples = df_test.drop(columns=[\"Label\", \"Attack\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              0         1         2         3         4         5         6    \\\n",
            "0       -0.186131  0.089346  0.234351  0.499705  0.611004 -0.108287 -0.442813   \n",
            "1       -0.186131  0.089346  0.234351  0.499705  0.611004 -0.108287 -0.442813   \n",
            "2       -0.186131  0.089346  0.234351  0.499705  0.611004 -0.108287 -0.442813   \n",
            "3       -0.186131  0.089346  0.234351  0.499705  0.611004 -0.108287 -0.442813   \n",
            "4       -0.186131  0.089346  0.234351  0.499705  0.611004 -0.108287 -0.442813   \n",
            "...           ...       ...       ...       ...       ...       ...       ...   \n",
            "1132126 -0.184152  0.098381  0.238208  0.498316  0.594795 -0.112433 -0.433364   \n",
            "1132127 -0.193824  0.122057  0.248642  0.508490  0.580410 -0.082905 -0.450611   \n",
            "1132128 -0.182810  0.094784  0.238498  0.495648  0.605904 -0.114430 -0.437895   \n",
            "1132129 -0.185517  0.101051  0.242561  0.496381  0.593074 -0.108055 -0.436051   \n",
            "1132130 -0.183881  0.095285  0.239258  0.496412  0.599172 -0.112877 -0.434997   \n",
            "\n",
            "              7         8         9    ...       246       247       248  \\\n",
            "0       -0.095080  0.068683  0.096496  ... -0.294899 -0.797715 -0.100322   \n",
            "1       -0.095080  0.068683  0.096496  ... -0.294899 -0.797715 -0.100322   \n",
            "2       -0.095080  0.068683  0.096496  ... -0.294899 -0.797715 -0.100322   \n",
            "3       -0.095080  0.068683  0.096496  ... -0.294899 -0.797715 -0.100322   \n",
            "4       -0.095080  0.068683  0.096496  ... -0.294899 -0.797715 -0.100322   \n",
            "...           ...       ...       ...  ...       ...       ...       ...   \n",
            "1132126 -0.103794  0.081493  0.103923  ... -0.269213 -0.767605 -0.097207   \n",
            "1132127 -0.129895  0.072036  0.084988  ... -0.263861 -0.750609 -0.085653   \n",
            "1132128 -0.095739  0.077718  0.100935  ... -0.282194 -0.782576 -0.100595   \n",
            "1132129 -0.107008  0.074640  0.099271  ... -0.272282 -0.768193 -0.098708   \n",
            "1132130 -0.101127  0.076996  0.102990  ... -0.275851 -0.775182 -0.099903   \n",
            "\n",
            "              249       250       251       252       253       254       255  \n",
            "0        0.261442  0.077142 -0.006085  0.272703 -0.072545 -0.090213 -0.200285  \n",
            "1        0.261442  0.077142 -0.006085  0.272703 -0.072545 -0.090213 -0.200285  \n",
            "2        0.261442  0.077142 -0.006085  0.272703 -0.072545 -0.090213 -0.200285  \n",
            "3        0.261442  0.077142 -0.006085  0.272703 -0.072545 -0.090213 -0.200285  \n",
            "4        0.261442  0.077142 -0.006085  0.272703 -0.072545 -0.090213 -0.200285  \n",
            "...           ...       ...       ...       ...       ...       ...       ...  \n",
            "1132126  0.261076  0.084027 -0.018970  0.290114 -0.053984 -0.081132 -0.184239  \n",
            "1132127  0.268117  0.069618 -0.047389  0.312055 -0.021952 -0.069776 -0.216119  \n",
            "1132128  0.260852  0.081167 -0.012410  0.278905 -0.065095 -0.088101 -0.185128  \n",
            "1132129  0.261356  0.081911 -0.024420  0.287187 -0.052486 -0.081580 -0.187169  \n",
            "1132130  0.260799  0.082711 -0.016154  0.283045 -0.059702 -0.085563 -0.185345  \n",
            "\n",
            "[1132131 rows x 256 columns]\n"
          ]
        }
      ],
      "source": [
        "print(test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "62BUDLtO4mla"
      },
      "outputs": [],
      "source": [
        "contamination = [0.001, 0.01, 0.04, 0.05, 0.1, 0.2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2i48uLj74mla",
        "outputId": "a0dd33ee-824d-4328-a960-e656e3aaf0ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/36 [00:00<?, ?it/s]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "  3%|▎         | 1/36 [02:24<1:24:05, 144.16s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "  6%|▌         | 2/36 [04:30<1:15:36, 133.41s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "  8%|▊         | 3/36 [06:37<1:11:58, 130.87s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 11%|█         | 4/36 [08:28<1:05:28, 122.76s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 14%|█▍        | 5/36 [10:23<1:01:58, 119.96s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 17%|█▋        | 6/36 [12:20<59:35, 119.18s/it]  /users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 19%|█▉        | 7/36 [15:05<1:04:50, 134.16s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 22%|██▏       | 8/36 [17:54<1:07:45, 145.21s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 25%|██▌       | 9/36 [20:21<1:05:33, 145.69s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 28%|██▊       | 10/36 [22:46<1:02:59, 145.38s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 31%|███       | 11/36 [25:11<1:00:33, 145.35s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 33%|███▎      | 12/36 [27:39<58:25, 146.08s/it]  /users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 36%|███▌      | 13/36 [32:05<1:09:54, 182.35s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 39%|███▉      | 14/36 [36:41<1:17:17, 210.79s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 42%|████▏     | 15/36 [40:14<1:14:03, 211.59s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 44%|████▍     | 16/36 [43:49<1:10:48, 212.40s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 47%|████▋     | 17/36 [47:42<1:09:14, 218.68s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 50%|█████     | 18/36 [51:33<1:06:41, 222.30s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 53%|█████▎    | 19/36 [57:09<1:12:39, 256.45s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 56%|█████▌    | 20/36 [1:02:59<1:15:53, 284.58s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 58%|█████▊    | 21/36 [1:08:23<1:14:06, 296.45s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 61%|██████    | 22/36 [1:14:05<1:12:21, 310.08s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 64%|██████▍   | 23/36 [1:19:36<1:08:34, 316.48s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 67%|██████▋   | 24/36 [1:25:06<1:04:05, 320.42s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 69%|██████▉   | 25/36 [1:31:53<1:03:29, 346.29s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 72%|███████▏  | 26/36 [1:37:39<57:42, 346.24s/it]  /users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 75%|███████▌  | 27/36 [1:43:26<51:59, 346.65s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 78%|███████▊  | 28/36 [1:49:13<46:14, 346.81s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 81%|████████  | 29/36 [1:55:02<40:32, 347.44s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 83%|████████▎ | 30/36 [2:00:32<34:12, 342.09s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 86%|████████▌ | 31/36 [2:06:55<29:32, 354.50s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 89%|████████▉ | 32/36 [2:13:26<24:21, 365.35s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 92%|█████████▏| 33/36 [2:19:28<18:12, 364.20s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 94%|█████████▍| 34/36 [2:25:26<12:05, 362.57s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            " 97%|█████████▋| 35/36 [2:32:50<06:26, 386.78s/it]/users/pg23/anshulsharma/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "100%|██████████| 36/36 [2:41:21<00:00, 268.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': 3, 'con': 0.001}\n",
            "0.9446504908193768\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9772    0.9987    0.9878    996643\n",
            "           1     0.9882    0.8287    0.9015    135488\n",
            "\n",
            "    accuracy                         0.9783   1132131\n",
            "   macro avg     0.9827    0.9137    0.9447   1132131\n",
            "weighted avg     0.9785    0.9783    0.9775   1132131\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyod.models.cblof import CBLOF\n",
        "n_est = [2,3,5,7,9,10]\n",
        "params = list(itertools.product(n_est, contamination))\n",
        "score = -1\n",
        "bs = None\n",
        "for n_est, con in tqdm(params):\n",
        "    \n",
        "    clf_if = CBLOF(n_clusters=n_est, contamination=con)\n",
        "    clf_if.fit(benign_train_samples)\n",
        "    y_pred = clf_if.predict(test_samples)\n",
        "    test_pred = y_pred\n",
        "\n",
        "    f1 = f1_score(test_labels, test_pred, average='macro')\n",
        "\n",
        "    if f1 > score:\n",
        "        score = f1\n",
        "        best_params = {'n_estimators': n_est,\n",
        "                       \"con\": con\n",
        "                }\n",
        "        bs = test_pred\n",
        "    del clf_if\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "print(best_params)\n",
        "print(score)\n",
        "print(classification_report(test_labels, bs, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "print(dgi.load_state_dict(torch.load('best_dgi.pkl')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DGI(\n",
              "  (encoder): SAGE(\n",
              "    (layers): ModuleList(\n",
              "      (0): SAGELayer(\n",
              "        (W_apply): Linear(in_features=78, out_features=128, bias=True)\n",
              "        (W_edge): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (discriminator): Discriminator()\n",
              "  (loss): BCEWithLogitsLoss()\n",
              ")"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dgi.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'local_scope'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/users/pg23/anshulsharma/fml/code.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     predictions \u001b[39m=\u001b[39m dgi(\u001b[39m3\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,test_samples[\u001b[39m0\u001b[39;49m])\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/users/pg23/anshulsharma/fml/code.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, n_features, e_features):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m   positive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(g, n_features, e_features, corrupt\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m   negative \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(g, n_features, e_features, corrupt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m   positive \u001b[39m=\u001b[39m positive[\u001b[39m1\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/users/pg23/anshulsharma/fml/code.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39m#nfeats = nfeats[n_perm]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m   \u001b[39m#nfeats = layer(g, nfeats, efeats)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m   nfeats, e_feats \u001b[39m=\u001b[39m layer(g, nfeats, efeats)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#return nfeats.sum(1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m nfeats\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m), e_feats\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/users/pg23/anshulsharma/fml/code.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g_dgl, nfeats, efeats):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mwith\u001b[39;00m g_dgl\u001b[39m.\u001b[39;49mlocal_scope():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     g \u001b[39m=\u001b[39m g_dgl\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu1.cse.iitb.ac.in/users/pg23/anshulsharma/fml/code.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m nfeats\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'local_scope'"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    predictions = dgi(3,0.1,test_samples[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
